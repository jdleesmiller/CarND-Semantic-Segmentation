{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# main.py resets stdout, so keep a reference here\n",
    "import sys\n",
    "stdout = sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from main import *\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Setup\n",
    "\n",
    "## Example Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_example_images(num_images):\n",
    "    get_batches_fn = helper.gen_batch_function(\n",
    "        os.path.join(DATA_DIR, 'data_road/training'), IMAGE_SHAPE)\n",
    "    \n",
    "    fig, axes = plt.subplots(\n",
    "        num_images, 3,\n",
    "        figsize=(15, 2*num_images))\n",
    "\n",
    "    i = 0\n",
    "    for inputs, labels in itertools.islice(get_batches_fn(1), num_images):\n",
    "        axes[i][0].imshow(inputs[0])\n",
    "        road_mask = labels[0,:,:,1]\n",
    "        axes[i][1].imshow(road_mask, cmap='gray')\n",
    "        axes[i][2].imshow(inputs[0] * road_mask[:,:,np.newaxis])\n",
    "        i += 1\n",
    "show_example_images(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "## Train / Validation Split\n",
    "\n",
    "The `helper.gen_batch_function` returns a generator that returns the images in shuffled order, so we just have to cut the total training set into a training set and a validation set; we use a roughly 80/20 split. Pickle the data for fast access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAINING_FILE = os.path.join(DATA_DIR, 'train_1.pickle')\n",
    "VALIDATION_FILE = os.path.join(DATA_DIR, 'validation_1.pickle')\n",
    "\n",
    "NUM_TOTAL_TRAINING_IMAGES = 289\n",
    "NUM_TRAINING_IMAGES = 230\n",
    "NUM_VALIDATION_IMAGES = NUM_TOTAL_TRAINING_IMAGES - NUM_TRAINING_IMAGES\n",
    "\n",
    "def save_input_and_labels(pathname, inputs, labels):\n",
    "    with open(pathname, 'wb') as file:\n",
    "        pickle.dump({\n",
    "            'inputs': inputs,\n",
    "            'labels': labels\n",
    "        }, file, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def generate_training_validation_split():\n",
    "    if os.path.exists(TRAINING_FILE):\n",
    "        print('Training set already exists.')\n",
    "        return\n",
    "    \n",
    "    get_batches_fn = helper.gen_batch_function(\n",
    "        os.path.join(DATA_DIR, 'data_road/training'), IMAGE_SHAPE)\n",
    "    \n",
    "    training_inputs = []\n",
    "    training_labels = []\n",
    "    validation_inputs = []\n",
    "    validation_labels = []\n",
    "    i = 0\n",
    "    for inputs, labels in get_batches_fn(1):\n",
    "        if i < NUM_TRAINING_IMAGES:\n",
    "            training_inputs.append(inputs[0])\n",
    "            training_labels.append(labels[0])\n",
    "        else:\n",
    "            validation_inputs.append(inputs[0])\n",
    "            validation_labels.append(labels[0])\n",
    "        i += 1\n",
    "        \n",
    "    save_input_and_labels(\n",
    "        TRAINING_FILE,\n",
    "        np.array(training_inputs),\n",
    "        np.array(training_labels))\n",
    "    save_input_and_labels(\n",
    "        VALIDATION_FILE,\n",
    "        np.array(validation_inputs),\n",
    "        np.array(validation_labels))\n",
    "        \n",
    "generate_training_validation_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_input_and_labels(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data['inputs'], data['labels']\n",
    "        \n",
    "TRAINING_INPUTS, TRAINING_LABELS = load_input_and_labels(TRAINING_FILE)\n",
    "VALIDATION_INPUTS, VALIDATION_LABELS = load_input_and_labels(VALIDATION_FILE)\n",
    "\n",
    "# for testing\n",
    "# TRAINING_INPUTS = TRAINING_INPUTS[:5,:,:,:]\n",
    "# TRAINING_LABELS = TRAINING_LABELS[:5,:,:,:]\n",
    "# VALIDATION_INPUTS = VALIDATION_INPUTS[:3,:,:,:]\n",
    "# VALIDATION_LABELS = VALIDATION_LABELS[:3,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[TRAINING_INPUTS.shape, VALIDATION_INPUTS.shape]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# From http://stackoverflow.com/a/40623158/2053820\n",
    "def dict_product(dicts):\n",
    "    \"\"\"\n",
    "    >>> list(dict_product(dict(number=[1,2], character='ab')))\n",
    "    [{'character': 'a', 'number': 1},\n",
    "     {'character': 'a', 'number': 2},\n",
    "     {'character': 'b', 'number': 1},\n",
    "     {'character': 'b', 'number': 2}]\n",
    "    \"\"\"\n",
    "    return (dict(zip(dicts, x)) for x in itertools.product(*dicts.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "GRID_FILE = os.path.join(DATA_DIR, 'grid_2.pickle')\n",
    "\n",
    "def run_grid_point(params):\n",
    "    params = params.copy()\n",
    "    batch_size = params.pop('batch_size')\n",
    "    max_epochs_without_progress = params.pop('max_epochs_without_progress')\n",
    "    max_epochs = params.pop('max_epochs')\n",
    "    keep_prob_value = params.pop('keep_prob')\n",
    "    learning_rate_value = params.pop('learning_rate')\n",
    "    \n",
    "    num_training_batches = \\\n",
    "        int(math.ceil(TRAINING_INPUTS.shape[0] / batch_size))\n",
    "    num_validation_batches = \\\n",
    "        int(math.ceil(VALIDATION_INPUTS.shape[0] / batch_size))\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        train_op, cross_entropy_loss, image_input, correct_label, \\\n",
    "            keep_prob, learning_rate, logits = build(sess, params)\n",
    "            \n",
    "        mean_iou, mean_iou_update_op = tf.metrics.mean_iou(\n",
    "            tf.reshape(correct_label[:,:,:,1], [-1]),\n",
    "            tf.nn.softmax(logits)[:,1] > 0.5,\n",
    "            NUM_CLASSES)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        \n",
    "        start = timer()\n",
    "        epochs_without_progress = 0\n",
    "        best_epoch = 0\n",
    "        best_validation_loss = math.inf\n",
    "        best_mean_iou = 0\n",
    "        for epoch in range(max_epochs):\n",
    "            if epochs_without_progress > max_epochs_without_progress:\n",
    "                break\n",
    "            \n",
    "            for batch in range(num_training_batches):\n",
    "                batch_start = batch * batch_size\n",
    "                batch_end = batch_start + batch_size\n",
    "                batch_inputs = TRAINING_INPUTS[batch_start:batch_end,:,:,:]\n",
    "                batch_labels = TRAINING_LABELS[batch_start:batch_end,:,:,:]\n",
    "                training_loss, _ = sess.run(\n",
    "                    [cross_entropy_loss, train_op], {\n",
    "                        image_input: batch_inputs,\n",
    "                        correct_label: batch_labels,\n",
    "                        keep_prob: keep_prob_value,\n",
    "                        learning_rate: learning_rate_value\n",
    "                    }\n",
    "                )\n",
    "                print('training', epoch, batch, training_loss)\n",
    "            \n",
    "            validation_loss = 0\n",
    "            validation_mean_iou = 0\n",
    "            for batch in range(num_validation_batches):\n",
    "                batch_start = batch * batch_size\n",
    "                batch_end = batch_start + batch_size\n",
    "                batch_inputs = VALIDATION_INPUTS[batch_start:batch_end,:,:,:]\n",
    "                batch_labels = VALIDATION_LABELS[batch_start:batch_end,:,:,:]\n",
    "                batch_loss, batch_mean_iou, _ = sess.run(\n",
    "                    [cross_entropy_loss, mean_iou, mean_iou_update_op], {\n",
    "                        image_input: batch_inputs,\n",
    "                        correct_label: batch_labels,\n",
    "                        keep_prob: 1.0\n",
    "                    }\n",
    "                )\n",
    "                actual_batch_size = batch_inputs.shape[0]\n",
    "                validation_loss += actual_batch_size * batch_loss\n",
    "                validation_mean_iou += actual_batch_size * batch_mean_iou\n",
    "            \n",
    "            validation_loss /= NUM_VALIDATION_IMAGES\n",
    "            validation_mean_iou /= NUM_VALIDATION_IMAGES\n",
    "\n",
    "            print('validation', epoch, validation_loss, validation_mean_iou)\n",
    "            \n",
    "            if validation_loss < best_validation_loss:\n",
    "                best_epoch = epoch\n",
    "                best_validation_loss = validation_loss\n",
    "                best_mean_iou = validation_mean_iou\n",
    "                epochs_without_progress = 0\n",
    "            else:\n",
    "                epochs_without_progress += 1\n",
    "            \n",
    "        return {\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_validation_loss': best_validation_loss,\n",
    "            'best_mean_iou': best_mean_iou,\n",
    "            'time': timer() - start,\n",
    "        }\n",
    "            \n",
    "def run_grid():\n",
    "    if os.path.isfile(GRID_FILE):\n",
    "        with open(GRID_FILE, 'rb') as f:\n",
    "            results = pickle.load(f)\n",
    "    else:\n",
    "        results = {}\n",
    "        \n",
    "    params_dict = {\n",
    "        'batch_size': [13],\n",
    "        'max_epochs_without_progress': [3],\n",
    "        'max_epochs': [50],\n",
    "        'keep_prob': [0.5],\n",
    "        'learning_rate': [0.001, 0.0001, 0.00001],\n",
    "        'kernel_size_3': [8, 16],\n",
    "        'kernel_size_4': [2, 4],\n",
    "        'kernel_size_7': [2, 4],\n",
    "        'conv_1x1_depth': [0, 2048, 4096]\n",
    "    }\n",
    "    \n",
    "    for params in dict_product(params_dict):\n",
    "        print(params)\n",
    "        \n",
    "        frozen_key = frozenset(params.items())\n",
    "        if frozen_key in results:\n",
    "            continue\n",
    "            \n",
    "        results[frozen_key] = run_grid_point(params)\n",
    "        \n",
    "        with open(GRID_FILE, 'wb') as f:\n",
    "            pickle.dump(results, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "run_grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def summarize_grid(file):\n",
    "    with open(file, 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "        \n",
    "    print(results)\n",
    "    \n",
    "summarize_grid(GRID_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
